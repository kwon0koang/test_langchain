{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langserve import RemoteRunnable\n",
    "import json\n",
    "import bs4\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경변수 로드 (.env)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# llm = OllamaFunctions(model=\"EEVE-Korean-Instruct-10.8B-v1.0:latest\", format=\"json\", temperature=0)\n",
    "llm = ChatOllama(model=\"EEVE-Korean-Instruct-10.8B-v1.0:latest\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Library/Caches/pypoetry/virtualenvs/test-langchain-lw2NDlv9-py3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/user/Library/Caches/pypoetry/virtualenvs/test-langchain-lw2NDlv9-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': 'cpu'}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")\n",
    "\n",
    "# 로컬 DB 불러오기\n",
    "MY_NEWS_INDEX = \"MY_NEWS_INDEX\"\n",
    "vectorstore1 = FAISS.load_local(MY_NEWS_INDEX, \n",
    "                               embeddings, \n",
    "                               allow_dangerous_deserialization=True)\n",
    "retriever1 = vectorstore1.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}) # 유사도 높은 3문장 추출\n",
    "MY_PDF_INDEX = \"MY_PDF_INDEX\"\n",
    "vectorstore2 = FAISS.load_local(MY_PDF_INDEX, \n",
    "                               embeddings, \n",
    "                               allow_dangerous_deserialization=True)\n",
    "retriever2 = vectorstore2.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}) # 유사도 높은 3문장 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='web_search', description='엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다. 엔비디아, 퍼플렉시티, 라마3 관련 정보는 이 도구를 사용해야 한다', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x10f5fd940>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x294389890>, search_kwargs={'k': 3}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x10f5fdb20>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x294389890>, search_kwargs={'k': 3}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')),\n",
       " Tool(name='pdf_search', description='생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다. 생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보는 이 도구를 사용해야 한다', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x10f5fd940>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x29729a8d0>, search_kwargs={'k': 3}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x10f5fdb20>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x29729a8d0>, search_kwargs={'k': 3}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "retriever_tool1 = create_retriever_tool(\n",
    "    retriever1,\n",
    "    name=\"saved_news_search\",\n",
    "    description=\"엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다. 엔비디아, 퍼플렉시티, 라마3 관련 정보는 이 도구를 사용해야 한다\",\n",
    ")\n",
    "\n",
    "retriever_tool2 = create_retriever_tool(\n",
    "    retriever2,\n",
    "    name=\"pdf_search\",\n",
    "    description=\"생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다. 생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보는 이 도구를 사용해야 한다\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool1, retriever_tool2]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_select_tool = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You have \"tools\" that can answer \"question\".\n",
    "Using \"tools\" as a guide, choose a \"tool\" that can answer \"question\".\n",
    "Without saying anything else, say the \"tool_name\" of the selected \"tool\" in English.\n",
    "If there is no appropriate 'tool', say 'None'.\n",
    "\n",
    "<tools>\n",
    "{tools}\n",
    "</tools>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "# answer :\n",
    "\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "def get_tools(query):\n",
    "    tool_info = [{\"tool_name\": tool.name, \"tool_description\": tool.description} for tool in tools]\n",
    "    print(f\"get_tools / {tool_info}\")\n",
    "    return json.dumps(tool_info, ensure_ascii=False)\n",
    "\n",
    "chain_for_select_tool = (\n",
    "    {\"tools\": get_tools, \"question\": RunnablePassthrough()}\n",
    "    | prompt_for_select_tool \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_tools / [{'tool_name': 'web_search', 'tool_description': '엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다. 엔비디아, 퍼플렉시티, 라마3 관련 정보는 이 도구를 사용해야 한다'}, {'tool_name': 'pdf_search', 'tool_description': '생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다. 생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보는 이 도구를 사용해야 한다'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'web_search'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"라마3 성능은?\"\n",
    "# query = \"생성형 AI 도입에 따른 규제 연구 책임자는?\"\n",
    "selected_tool = chain_for_select_tool.invoke(query)\n",
    "selected_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_retriever_by_tool_name / name: pdf_search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='15\\n나. 생성형 AI 규제 유형화\\n○본 연구에서는 주요국의 생성형 AI 규제 동향을 ‘AI 관련 입법 목적’ \\n및 ‘AI에 대한 규제는 규제원칙 및 방식’에 따라 아래와 같이 분류한다.\\n○다수 국가들이 AI 규제를 준비하고 있는 가운데, 미국과 EU는 AI에 대\\n한 규제 입법에 앞장서 왔다. 미국과 EU의 규제 내용 및 동향 대한 자\\n세한 내용은 4장에서 다룰 것이다.\\n- 2019년 트럼프 전 대통령이 행정명령 제13859호에서 “미국 인공지\\n능 구상”을 공표한 이후, 미국은 AI 관련 규제를 입법하였다. 본 연\\n구에서는 AI 연구개발과 훈련과 관련한 법률인 「생성적 적대 신경\\n망 출력물 확인법」, 「국가 인공지능 구상법 2020」을 분석한다. 또\\n한 연방정부가 제정한 AI 사용에 관한 행정명령 제13960호, 「정부인\\n공지능법 2020」, 「조달인력의 인공지능 역량강화법」, 「미국인공\\n지능진흥법」 등과 더불어 최근 논의되고 있는 생성형 AI 기술 규제', metadata={'source': '/Users/user/kykdev/7000_AI/test_langchain/assets/생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf', 'file_path': '/Users/user/kykdev/7000_AI/test_langchain/assets/생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf', 'page': 20, 'total_pages': 227, 'format': 'PDF 1.4', 'title': '', 'author': 'fpqlt', 'subject': '', 'keywords': '', 'creator': 'Hwp 2018 10.0.0.9139', 'producer': 'Hancom PDF 1.3.0.538', 'creationDate': \"D:20231228104945+09'00'\", 'modDate': \"D:20231228104945+09'00'\", 'trapped': ''}),\n",
       " Document(page_content='의 법적 책임이 불분명한 상황이다. 다수의 국가가 AI 규제를 준비하는 \\n가운데, 미국과 EU를 중심으로 생성형 AI에 대한 규제 입법에 나서고 \\n있다.\\n○본 연구는 생성형 AI 관련 제도를 주도하는 국가의 사례 연구를 통해 \\n한국에서 발생할 수 있는 다양한 문제에 대비하는 정책적 방향성을 설\\n정하는 것을 목적으로 한다.\\n나. 연구 수행 범위: 중범위 메타 분석\\n○시간적 범위\\n- 2000년대 이후 SNS(social media) 도입과 빅데이터의 선거 활용에 따\\n른 선거 영향 및 문제점을 분석한다.', metadata={'source': '/Users/user/kykdev/7000_AI/test_langchain/assets/생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf', 'file_path': '/Users/user/kykdev/7000_AI/test_langchain/assets/생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf', 'page': 6, 'total_pages': 227, 'format': 'PDF 1.4', 'title': '', 'author': 'fpqlt', 'subject': '', 'keywords': '', 'creator': 'Hwp 2018 10.0.0.9139', 'producer': 'Hancom PDF 1.3.0.538', 'creationDate': \"D:20231228104945+09'00'\", 'modDate': \"D:20231228104945+09'00'\", 'trapped': ''}),\n",
       " Document(page_content='ii\\n【2023년도 중앙선거관리위원회 정책연구용역 보고서】\\n『생성형 AI 신기술 도입에 따른 \\n선거 규제 연구』\\n연구책임자 : \\n김\\n 주\\n 희(국 립 부 경 대 학 교)\\n공동연구자 :\\n차\\n 재\\n 권(국 립 부 경 대 학 교)\\n김\\n 현\\n 정(동 아 대 학 교)\\n조\\n 성\\n 복(국 민 대 학 교)\\n연구보조원 : \\n박\\n 서\\n 현(국 립 부 경 대 학 교)\\n권\\n 수\\n 민(국 립 부 경 대 학 교)\\n본 연구보고서는 2023년도 중앙선거관리위원회 정책연구용역 과제로서 연구내용은 중앙\\n선거관리위원회의 공식 견해가 아님.', metadata={'source': '/Users/user/kykdev/7000_AI/test_langchain/assets/생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf', 'file_path': '/Users/user/kykdev/7000_AI/test_langchain/assets/생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf', 'page': 1, 'total_pages': 227, 'format': 'PDF 1.4', 'title': '', 'author': 'fpqlt', 'subject': '', 'keywords': '', 'creator': 'Hwp 2018 10.0.0.9139', 'producer': 'Hancom PDF 1.3.0.538', 'creationDate': \"D:20231228104945+09'00'\", 'modDate': \"D:20231228104945+09'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_retriever_by_tool_name(name) -> VectorStoreRetriever:\n",
    "    print(f\"get_retriever_by_tool_name / name: {name}\")\n",
    "    for tool in tools:\n",
    "        if tool.name == name:\n",
    "            # print(tool.func) # functools.partial(<function _get_relevant_documents at 0x1487dd6c0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x317e52ea0>, search_kwargs={'k': 5}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')\n",
    "            return tool.func.keywords['retriever']\n",
    "    return None\n",
    "\n",
    "retriever = get_retriever_by_tool_name(selected_tool)\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_tools / [('web_search', '엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다. 엔비디아, 퍼플렉시티, 라마3 관련 정보는 이 도구를 사용해야 한다'), ('pdf_search', '생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다. 생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보는 이 도구를 사용해야 한다')]\n",
      "get_retriever_by_tool_name / name: pdf_search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'생성형 AI 도입에 따른 규제 연구 책임자는 김주희 교수님입니다.\\n\\n[출처]\\n생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf (1페이지)\\n생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf (6페이지)\\n생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf (20페이지)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "너는 유능한 업무 보조자야.\n",
    "다음 context를 사용해서 question에 대한 답을 말해줘.\n",
    "정답을 모르면 모른다고만 해. 한국어로 대답해.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "# answer :\n",
    "\"\"\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "retrieved_docs = []\n",
    "def get_page_contents_with_metadata(docs) -> str: \n",
    "    global retrieved_docs\n",
    "    retrieved_docs = docs\n",
    "    \n",
    "    result = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        if i > 0:\n",
    "            result += \"\\n\\n\"\n",
    "        result += f\"## 본문: {doc.page_content}\\n### 출처: {doc.metadata['source']}\"\n",
    "    return result\n",
    "\n",
    "def get_retrieved_docs(query) -> str:\n",
    "    selected_tool = chain_for_select_tool.invoke(query)\n",
    "    retriever = get_retriever_by_tool_name(selected_tool)\n",
    "    docs = retriever.invoke(query)\n",
    "    return get_page_contents_with_metadata(docs)\n",
    "\n",
    "def get_metadata_sources(docs) -> str: \n",
    "    sources = set()\n",
    "    for doc in docs:\n",
    "        source = doc.metadata['source']\n",
    "        is_pdf = source.endswith('.pdf')\n",
    "        if (is_pdf):\n",
    "            file_path = doc.metadata['source']\n",
    "            file_name = os.path.basename(file_path)\n",
    "            source = f\"{file_name} ({int(doc.metadata['page']) + 1}페이지)\"\n",
    "        sources.add(source)\n",
    "    return \"\\n\".join(sources)\n",
    "\n",
    "def parse(ai_message: AIMessage) -> str:\n",
    "    \"\"\"Parse the AI message and add source.\"\"\"\n",
    "    return f\"{ai_message.content}\\n\\n[출처]\\n{get_metadata_sources(retrieved_docs)}\"\n",
    "\n",
    "agent_chain = (\n",
    "    {\"context\": get_retrieved_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parse\n",
    ")\n",
    "\n",
    "# agent_chain.invoke(\"퍼플렉시티가 투자받은 금액?\") # saved_news_search / '퍼플렉시티는 투자받은 금액이 약 6300만 달러(약 860억 원)이며, 회사 가치는 10억 달러(약 1조 3760억 원) 이상으로 평가받았습니다.'\n",
    "agent_chain.invoke(\"생성형 AI 도입에 따른 규제 연구 책임자는?\") # pdf_search / '해당 문서에 따르면, 생성형 AI 도입과 관련된 규제 연구를 담당하는 연구 책임자는 김주희 교수님입니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-langchain-lw2NDlv9-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
