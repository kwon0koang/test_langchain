{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langserve import RemoteRunnable\n",
    "import bs4\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.documents.base import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경변수 로드 (.env)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# llm = OllamaFunctions(model=\"EEVE-Korean-Instruct-10.8B-v1.0:latest\", format=\"json\", temperature=0)\n",
    "llm = ChatOllama(model=\"EEVE-Korean-Instruct-10.8B-v1.0:latest\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwon0koang/Library/Caches/pypoetry/virtualenvs/test-langchain-PJ4Uqdqm-py3.12/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/kwon0koang/Library/Caches/pypoetry/virtualenvs/test-langchain-PJ4Uqdqm-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\",\n",
    "    model_kwargs = {'device': 'cpu'}, # 모델이 CPU에서 실행되도록 설정. GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있음\n",
    "    encode_kwargs = {'normalize_embeddings': True}, # 임베딩 정규화. 모든 벡터가 같은 범위의 값을 갖도록 함. 유사도 계산 시 일관성을 높여줌\n",
    ")\n",
    "\n",
    "# 로컬 DB 불러오기\n",
    "MY_FAISS_INDEX = \"MY_FAISS_INDEX\"\n",
    "vectorstore1 = FAISS.load_local(MY_FAISS_INDEX, \n",
    "                               embeddings, \n",
    "                               allow_dangerous_deserialization=True)\n",
    "retriever1 = vectorstore1.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}) # 유사도 높은 5문장 추출\n",
    "MY_PDF_INDEX = \"MY_PDF_INDEX\"\n",
    "vectorstore2 = FAISS.load_local(MY_PDF_INDEX, \n",
    "                               embeddings, \n",
    "                               allow_dangerous_deserialization=True)\n",
    "retriever2 = vectorstore2.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}) # 유사도 높은 5문장 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='web_search', description='엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x12e246660>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x2bb6948f0>, search_kwargs={'k': 5}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x12e246b60>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x2bb6948f0>, search_kwargs={'k': 5}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')),\n",
       " Tool(name='pdf_search', description='생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x12e246660>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x2bb694a40>, search_kwargs={'k': 5}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x12e246b60>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x2bb694a40>, search_kwargs={'k': 5}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "retriever_tool1 = create_retriever_tool(\n",
    "    retriever1,\n",
    "    name=\"web_search\",\n",
    "    description=\"엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다\",\n",
    ")\n",
    "\n",
    "retriever_tool2 = create_retriever_tool(\n",
    "    retriever2,\n",
    "    name=\"pdf_search\",\n",
    "    description=\"생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool1, retriever_tool2]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_with_tools = llm.bind_tools(tools=tools)\n",
    "# response = llm_with_tools.invoke(\"What is 3*12=? Also, What is 11+43?\")\n",
    "# print(f\"response: {response}\")\n",
    "# print(f\"tool_calls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_select_tool = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "Select one ‘tool’ to indicate which tool you would use to answer the ‘question’ correctly.\n",
    "Say only the ‘name’ of the ‘tool’ without saying anything else.\n",
    "\n",
    "<tools>\n",
    "{tools}\n",
    "</tools>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "# answer :\n",
    "\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "def get_tools(query):\n",
    "    tool_info = [(tool.name, tool.description) for tool in tools]\n",
    "    print(f\"get_tools / {tool_info}\") # [('web_search', '엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다'), ('pdf_search', '생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다')]\n",
    "    return str(tool_info)\n",
    "\n",
    "chain_for_select_tool = (\n",
    "    {\"tools\": get_tools, \"question\": RunnablePassthrough()}\n",
    "    | prompt_for_select_tool \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_tools / [('web_search', '엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다'), ('pdf_search', '생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'web_search'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"라마3 성능은?\"\n",
    "selected_tool = chain_for_select_tool.invoke(query)\n",
    "selected_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_retriever_by_tool_name / name: web_search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"라마 3 벤치마크 결과 (사진=메타)\\n\\n\\n라마 3는 객관식 문제(MMLU)와 코딩(HumanEval)에는 강하지만, 70B의 경우 수학 단어 문제(MATH) 해결이나 대학원생 수준의 객관식 문제(GPQA)에서는 제미나이 프로 1.5에 떨어졌다.\\xa0\\n특히 인간 선호도에서 경쟁 모델을 앞서는 것으로 알려졌다.\\n조언 요청, 브레인스토밍, 분류, 비공개 질문 답변, 코딩, 창의적인 글쓰기, 추출, 공개 질문 답변, 추론, 재작성 및 요약 등 12가지 주요 사용 사례를 포함한 1800개\\xa0프롬프트\\xa0구축\\xa0데이터셋에 대한 인간 평가에서 오픈AI의 'GPT-3.5', 미스트랄 7B, 클로드 3 소네트보다 높게 평가됐다.\\n\\n\\n라마 3 인간 평가 결과 (사진=메타)\", metadata={'source': 'https://www.aitimes.com/news/articleView.html?idxno=158943'}),\n",
       " Document(page_content='라마 3 인간 평가 결과 (사진=메타)\\n\\n\\n허깅페이스에 따르면, 라마 3는 공개 후 몇시간만에 LLM 리더보드\\xa01위에 오르며 역대 가장 빠른 1위 달성 기록을 세웠다.\\n또 이전 라마 1과 2를 기반으로 3만개 이상의 새로운 모델이 출시됐으며, 라마 2 모델은 1700억번 다운로드됐다는 통계치도 공개해 눈길을 모았다.\\xa0\\n다만 라마 3는 완전한 오픈 소스가 아니다.\\xa0연구용 및 상업용으로 모두 사용할 수 있지만, 개발자가 다른 생성 모델을 훈련하기 위해 모델을 사용하는 것을 금지한다.\\n\\n\\n메타 AI (사진=메타)', metadata={'source': 'https://www.aitimes.com/news/articleView.html?idxno=158943'}),\n",
       " Document(page_content='특히 15조개 이상의 토큰을 동원, 학습량이 라마 2 대비 7배 이상 많으며 코드량은 4배 더 많다. 다만 데이터셋은 공개하지 않았다.\\n이후 미세조정에는 일상적인 질문부터 과학·기술·공학·수학(STEM), 코딩, 역사 지식에 이르기까지 다양한 분야의 데이터셋이 사용됐다. 훈련\\xa0규모를 확대하는 것은 물론, 고도화된 ‘지시 미세조정(instruction fine-tuning)’ 과정도 진행했다.\\xa0\\n또\\xa0라마 3는 라마 2보다 2배 큰 8000토큰의 컨텍스트 길이를 지원한다.\\n오픈 소스라는 점을 감안, 안전하고 책임감 있는 개발과 사용을 위한 다양한 안전장치도 마련했다고 밝혔다. 전문가와 자동화된 도구를 활용한 레드팀 테스트를 통해 부적절한 답변의 가능성을 최소화했다고 전했다.', metadata={'source': 'https://www.aitimes.com/news/articleView.html?idxno=158943'}),\n",
       " Document(page_content=\"메타는 이번에 공개한 두가지 크기의 라마 3 버전이 다양한 벤치마크 테스트에서 구글의 '젬마'나 '제미나이 프로 1.5', 미스트랄 AI의 '미스트랄 7B', 엔트로픽의 '클로드 3 소네트'와 같은 모델들을 능가한다고 주장했다.\", metadata={'source': 'https://www.aitimes.com/news/articleView.html?idxno=158943'}),\n",
       " Document(page_content='메타가 오픈 소스 대형언어모델(LLM) ‘라마 3’ 중 소형 버전 2개를 공개했다. 이에 대해 일론 머스크 테슬라 CEO는\\xa0\\xa0X(트위터)에 “나쁘지 않다(Not bad)”라고 평가했다.\\n메타는\\xa018일(현지시간) 라마 3 시리즈 중 매개변수 80억개(8B)와 700억개(70B)의 소형 버전을 공개했다. 메타는 매개변수 4000억개(400B)의 가장 큰 버전도 개발하고 있다고 밝혔다.\\n라마 3는 사전 훈련과 미세조정 과정을 고도화, 성능이\\xa0대폭 향상됐다.\\xa0사전 훈련을 위해 데이터셋의 양을 늘리고, 필터링 과정을 거쳐 고품질 데이터만을 선별했다.\\n특히 15조개 이상의 토큰을 동원, 학습량이 라마 2 대비 7배 이상 많으며 코드량은 4배 더 많다. 다만 데이터셋은 공개하지 않았다.', metadata={'source': 'https://www.aitimes.com/news/articleView.html?idxno=158943'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_retriever_by_tool_name(name) -> VectorStoreRetriever:\n",
    "    print(f\"get_retriever_by_tool_name / name: {name}\")\n",
    "    for tool in tools:\n",
    "        if tool.name == name:\n",
    "            # print(tool.func) # functools.partial(<function _get_relevant_documents at 0x1487dd6c0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x317e52ea0>, search_kwargs={'k': 5}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')\n",
    "            return tool.func.keywords['retriever']\n",
    "    return None\n",
    "\n",
    "retriever = get_retriever_by_tool_name(selected_tool)\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_tools / [('web_search', '엔비디아, 퍼플렉시티, 라마3 관련 정보를 검색한다'), ('pdf_search', '생성형 AI 신기술 도입에 따른 선거 규제 연구 관련 정보를 검색한다')]\n",
      "get_retriever_by_tool_name / name: pdf_search\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'해당 문서에 따르면, 생성형 AI 도입과 관련된 규제 연구를 담당하는 연구 책임자는 김주희 교수님입니다.\\n\\n[출처]\\n생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf (6페이지)\\n생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf (20페이지)\\n생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf (21페이지)\\n생성형_AI_신기술_도입에_따른_선거_규제_연구_결과보고서.pdf (1페이지)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "너는 유능한 업무 보조자야.\n",
    "다음 context를 사용해서 question에 대한 답을 말해줘.\n",
    "정답을 모르면 모른다고만 해.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "# answer :\n",
    "\"\"\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "retrieved_docs = []\n",
    "def get_page_contents_with_metadata(docs) -> str: \n",
    "    global retrieved_docs\n",
    "    retrieved_docs = docs\n",
    "    \n",
    "    result = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        if i > 0:\n",
    "            result += \"\\n\\n\"\n",
    "        result += f\"## 본문: {doc.page_content}\\n### 출처: {doc.metadata['source']}\"\n",
    "    return result\n",
    "\n",
    "def get_retrieved_docs(query) -> str:\n",
    "    selected_tool = chain_for_select_tool.invoke(query)\n",
    "    retriever = get_retriever_by_tool_name(selected_tool)\n",
    "    docs = retriever.invoke(query)\n",
    "    return get_page_contents_with_metadata(docs)\n",
    "\n",
    "def get_metadata_sources(docs) -> str: \n",
    "    sources = set()\n",
    "    for doc in docs:\n",
    "        source = doc.metadata['source']\n",
    "        is_pdf = source.endswith('.pdf')\n",
    "        if (is_pdf):\n",
    "            file_path = doc.metadata['source']\n",
    "            file_name = os.path.basename(file_path)\n",
    "            source = f\"{file_name} ({doc.metadata['page']}페이지)\"\n",
    "        sources.add(source)\n",
    "    return \"\\n\".join(sources)\n",
    "\n",
    "def parse(ai_message: AIMessage) -> str:\n",
    "    \"\"\"Parse the AI message and add source.\"\"\"\n",
    "    return f\"{ai_message.content}\\n\\n[출처]\\n{get_metadata_sources(retrieved_docs)}\"\n",
    "\n",
    "agent_chain = (\n",
    "    {\"context\": get_retrieved_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parse\n",
    ")\n",
    "\n",
    "# final_chain.invoke(\"퍼플렉시티가 투자받은 금액?\") # web_search / '퍼플렉시티는 투자받은 금액이 약 6300만 달러(약 860억 원)이며, 회사 가치는 10억 달러(약 1조 3760억 원) 이상으로 평가받았습니다.'\n",
    "agent_chain.invoke(\"생성형 AI 도입에 따른 규제 연구 책임자는?\") # pdf_search / '해당 문서에 따르면, 생성형 AI 도입과 관련된 규제 연구를 담당하는 연구 책임자는 김주희 교수님입니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-langchain-lw2NDlv9-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
