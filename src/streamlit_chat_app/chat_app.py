import os
import sys

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
# ìƒìœ„ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
# sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(os.path.dirname(__file__)))))

from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage
from langchain_openai import ChatOpenAI
import json
from langchain import hub
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
import streamlit as st
from streamlit.runtime.state import SessionStateProxy
from langchain_core.vectorstores import VectorStoreRetriever
import time
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableBranch
from langchain.tools.retriever import create_retriever_tool
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import List, Union
from langchain_community.tools import Tool
from langchain_core.documents.base import Document
from datetime import datetime
from utils import current_date, perform_groundedness_check, grounded_result_mapping
from callbacks import StreamCallback
from tools import tools, options_in_sidebar, TOOL_AUTO, SAVED_NEWS_SEARCH_TOOL_NAME, PDF_SEARCH_TOOL_NAME, WEB_SEARCH_TOOL_NAME

st.set_page_config(
    page_title="ê¶Œë´‡", # í˜ì´ì§€ ì œëª©
    page_icon=":robot_face:", # í˜ì´ì§€ ì•„ì´ì½˜
    layout="centered",
    initial_sidebar_state="auto"
)

st.title("ê¶Œë´‡ ğŸ¤–")

eeve = ChatOllama(model="EEVE-Korean-Instruct-10.8B-v1.0:latest", temperature=0)
# llama = ChatOllama(model="llama3:8b", temperature=0)
qwen2 = ChatOllama(model="qwen2:latest", temperature=0)

# ==========================================================================================================================================================================================

# ì˜µì…˜ëª… ì¶”ì¶œ
option_names, option_display_names = zip(*options_in_sidebar)

if 'selected_option_name' not in st.session_state:
    st.session_state.selected_option_name = None

if 'test_count' not in st.session_state:
    st.session_state.test_count = 12345

# ì„ íƒëœ ì˜µì…˜ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_selected_option():
    selected_option_display_name = st.session_state.selected_option_display_name
    for name, display_name in options_in_sidebar:
        if display_name == selected_option_display_name:
            st.session_state['selected_option_name'] = name
            break
    print(f"update_selected_option / selected_option_display_name: {selected_option_display_name} / selected_option_name: {st.session_state['selected_option_name']}")

# ì‚¬ì´ë“œë°”ì— selectbox ìƒì„±
selected_option_display_name = st.sidebar.selectbox(
    'ë„êµ¬ ğŸ› ï¸',
    option_display_names,
    on_change=update_selected_option,
    key='selected_option_display_name'
)

# ==========================================================================================================================================================================================

# ì í•©í•œ tool ì¶”ì¶œ ìœ„í•œ í”„ë¡¬í”„íŠ¸
prompt_for_extract_actions = hub.pull("kwonempty/extract-actions-for-ollama")

def get_tools(query):
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ì˜ ì´ë¦„ê³¼ ì„¤ëª…ì„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    """
    # tools ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë„êµ¬ì˜ ì´ë¦„, ì„¤ëª…ì„ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì¶”ì¶œ
    tool_info = [{"tool_name": tool.name, "tool_description": tool.description} for tool in tools]
    
    print(f"get_tools / tool_info: {tool_info}")
    
    # tool_info ë¦¬ìŠ¤íŠ¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
    return json.dumps(tool_info, ensure_ascii=False)

chain_for_extract_actions = (
    {"tools": get_tools, "question": RunnablePassthrough()}
    | prompt_for_extract_actions 
    | qwen2
    | StrOutputParser()
    )

# ==========================================================================================================================================================================================

def get_retriever_by_tool_name(name: str) -> VectorStoreRetriever:
    """
    ë„êµ¬ ì´ë¦„ì„ í†µí•´ ê²€ìƒ‰ê¸° ë°˜í™˜
    """
    for tool in tools:
        if tool.name == name:
            return tool.func.keywords['retriever']
    return None
    
def get_documents_from_actions(actions_json: str, tools: List[Tool]) -> List[Document]:
    """
    ì£¼ì–´ì§„ JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ í•´ë‹¹ ì•¡ì…˜ì— ëŒ€ì‘í•˜ëŠ” ê²€ìƒ‰ê¸°ë¥¼ ì°¾ì•„ì„œ 
    ì•¡ì…˜ì„ ì‹¤í–‰ í›„ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°˜í™˜
    
    :param actions_json: ì•¡ì…˜ê³¼ ê·¸ ì…ë ¥ì´ í¬í•¨ëœ JSON ë¬¸ìì—´
    :param tools: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    :return: ì•¡ì…˜ì„ í†µí•´ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    print(f"get_documents_from_actions / actions_json: {actions_json}")
    
    # JSON ë¬¸ìì—´ì„ íŒŒì‹±
    try:
        actions = json.loads(actions_json)
    except json.JSONDecodeError:
        raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ JSON ë¬¸ìì—´")

    # íŒŒì‹±ëœ ê°ì²´ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
    if not isinstance(actions, list):
        raise ValueError("ì œê³µëœ JSONì€ ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‚˜íƒ€ë‚´ì•¼ í•¨")

    documents = []

    # ê° ì•¡ì…˜ì„ ì²˜ë¦¬
    for action in actions:
        if not isinstance(action, dict) or 'action' not in action or 'action_input' not in action:
            continue  # ìœ íš¨í•˜ì§€ ì•Šì€ ì•¡ì…˜ì€ ê±´ë„ˆëœ€

        tool_name = action['action']
        action_input = action['action_input']
        print(f"get_documents_from_actions / tool_name: {tool_name} / action_input: {action_input}")
        
        if tool_name == "None": # ì‚¬ìš©í•  ë„êµ¬ ì—†ìŒ. ë°”ë¡œ ë¹ˆ document ë¦¬í„´
            print(f"get_documents_from_actions / ì‚¬ìš©í•  ë„êµ¬ ì—†ìŒ. ë°”ë¡œ ë¹ˆ document ë¦¬í„´")
            return []
        
        retriever = get_retriever_by_tool_name(tool_name)
        
        if retriever:
            # ì•¡ì…˜ ì…ë ¥ìœ¼ë¡œ ê²€ìƒ‰ê¸° ì‹¤í–‰
            retrieved_docs = retriever.invoke(action_input)
            documents.extend(retrieved_docs)
        
    print(f"get_documents_from_actions / len(documents): {len(documents)}")
    return documents


# ==========================================================================================================================================================================================

agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë„ˆëŠ” ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ìœ ëŠ¥í•œ ì—…ë¬´ ë³´ì¡°ìì•¼.
ì•„ë˜ì˜ contextë¥¼ ì‚¬ìš©í•´ì„œ questionì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ì¤˜.

ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•´.
2. contextì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•´ì„œ ë‹µë³€í•´ì•¼ í•´.
3. ì •ë‹µì„ í™•ì‹¤íˆ ì•Œ ìˆ˜ ì—†ë‹¤ë©´ "ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë§í•´.
4. ë‹µë³€ ì‹œ ì¶”ì¸¡í•˜ê±°ë‚˜ ê°œì¸ì ì¸ ì˜ê²¬ì„ ì¶”ê°€í•˜ì§€ ë§ˆ.
5. ê°€ëŠ¥í•œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•´.
""")
    , MessagesPlaceholder(variable_name="messages"),
    ("human", """
     
# question: 
{question}

# context: 
{context}

# answer: 
""")
])

default_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë„ˆëŠ” ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ìœ ëŠ¥í•œ ì—…ë¬´ ë³´ì¡°ìì•¼.
ë‹¤ìŒ ì§ˆë¬¸ì— ìµœì„ ì„ ë‹¤í•´ì„œ ëŒ€ë‹µí•´ì¤˜.
"""
    )
    , MessagesPlaceholder(variable_name="messages")
    , ("human", "{question}")
])

if 'retrieved_docs' not in st.session_state:
    st.session_state.retrieved_docs = []
    
def get_page_contents_string(docs) -> str: 
    """
    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ê° ë¬¸ì„œì˜ ë³¸ë¬¸ ë‚´ìš©ì„ ì´ì–´ë¶™ì¸ ë¬¸ìì—´ì„ ìƒì„±
    """
    st.session_state.retrieved_docs = docs
    
    result = ""
    
    for i, doc in enumerate(docs):
        if i > 0:
            result += "\n"
            
        # if 'url' in doc:
        #     # Web ê²€ìƒ‰
        #     result += f"## ë³¸ë¬¸: {doc['content']}\n### ì¶œì²˜: {doc['url']}"
        # else:
        #     # Vector DB ê²€ìƒ‰
        #     result += f"## ë³¸ë¬¸: {doc.page_content}\n### ì¶œì²˜: {doc.metadata['source']}"
        
        # LLM ë‹µë³€ ì´í›„ì— parse í•¨ìˆ˜ë¡œ ì¶œì²˜ ë¶™ì—¬ì¤„ê±°ë‹ˆê¹Œ ë³¸ë¬¸ë§Œ ì´ì–´ë¶™ì¸ ë¬¸ìì—´ ìƒì„±í•˜ì
        if 'url' in doc:
            # Web ê²€ìƒ‰
            result += f"{doc['content']}"
        else:
            # Vector DB ê²€ìƒ‰
            result += f"{doc.page_content}"
    
    return result

def check_context(inputs: dict) -> bool:
    """
    context ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    
    :return: ë¬¸ìì—´ì´ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ True, ë¹„ì–´ìˆìœ¼ë©´ False
    """
    result = bool(inputs['context'].strip())
    print(f"check_context / result: {result}")
    return result

def retrieved_docs_and_get_messages(messages: List[BaseMessage], selected_option_name: str) -> dict:
    """
    ì¿¼ë¦¬ì— ë”°ë¼ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , í•´ë‹¹ ë¬¸ì„œë“¤ì˜ ë³¸ë¬¸ ë‚´ìš©ê³¼ ì¶œì²˜ë¥¼ í¬í•¨í•œ ë¬¸ìì—´ì„ ë°˜í™˜
    """
    print("========================")
    print(f"retrieved_docs_and_get_messages / messages: {messages}")
    query = messages[-1].content # last human message
    print(f"retrieved_docs_and_get_messages / query: {query}")
    
    if TOOL_AUTO == selected_option_name:
        actions_json = chain_for_extract_actions.invoke(query)
        st.session_state.retrieved_docs = get_documents_from_actions(actions_json, tools)
    else:
        retriever = get_retriever_by_tool_name(selected_option_name)
        st.session_state.retrieved_docs = retriever.invoke(query)
    
    messages_without_last = messages[:-1]
    
    if len(st.session_state.retrieved_docs) <= 0:
        return {"messages": messages_without_last
            , "context": ""
            , "question": query}
    
    return {"messages": messages_without_last
            , "context": get_page_contents_string(st.session_state.retrieved_docs)
            , "question": query}

def get_metadata_sources(docs) -> str: 
    """
    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì„œì˜ ì¶œì²˜ ì¶”ì¶œí•´ì„œ ë¬¸ìì—´ë¡œ ë°˜í™˜
    """
    sources = set()
    
    for doc in docs:
        if 'url' in doc:
            # Web ê²€ìƒ‰
            sources.add(doc['url'])
        else:
            # Vector DB ê²€ìƒ‰
            source = doc.metadata['source']
            is_pdf = source.endswith('.pdf')
            if (is_pdf):
                file_path = doc.metadata['source']
                file_name = os.path.basename(file_path)
                source = f"[{file_name} ({int(doc.metadata['page']) + 1}í˜ì´ì§€)](file://{file_path})"
            sources.add(source)
        
    return "\n\n".join(sources)

def parse(ai_message: AIMessage) -> str:
    """
    AI ë©”ì‹œì§€ íŒŒì‹±í•´ì„œ ë‚´ìš©ì— ì¶œì²˜ ì¶”ê°€
    """
    return f"{ai_message.content}\n\n<span style='color:gray;'>[ì¶œì²˜]</span>\n\n{get_metadata_sources(st.session_state.retrieved_docs)}"

with_context_chain = (
    RunnablePassthrough()
    | RunnableLambda(lambda x: {
        "messages": x["messages"]
        , "context": x["context"]
        , "question": x["question"]
        })
    | agent_prompt
    | eeve
    | parse
)

without_context_chain = (
    RunnablePassthrough()
    | RunnableLambda(lambda x: {
        "messages": x["messages"]
        ,"question": x["question"]
        })
    | default_prompt
    | eeve
    | StrOutputParser()
)

agent_chain = (
    RunnablePassthrough()
    | RunnableLambda(lambda x: retrieved_docs_and_get_messages(x["messages"], st.session_state.selected_option_name))
    | RunnableBranch(
        (lambda x: check_context(x), with_context_chain),
        without_context_chain  # default
    )
)

# ==========================================================================================================================================================================================

if "messages" not in st.session_state:
    # st.session_state.messages = [AIMessage(type="ai", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")]
    st.session_state.messages = []

# 10ê°œê¹Œì§€ë§Œ í‘œì‹œë˜ë„ë¡ ë©”ì‹œì§€ ìˆ˜ ì œí•œ
MAX_MESSAGES_COUNT = 10
if len(st.session_state.messages) >= MAX_MESSAGES_COUNT:
    st.session_state.messages = st.session_state.messages[2:]

for msg in st.session_state.messages:
    print(f"for msg in st.session_state.messages / msg.content: {msg.content}")
    st.chat_message(msg.type).markdown(msg.content
                                       , unsafe_allow_html=True
                                       )

query = st.chat_input()
if query:
    st.write("") # agent_chain ê²€ìƒ‰ ì¤‘ ë©”ì‹œì§€ ë„ìš¸ ë•Œ, ì´ì „ ë©”ì‹œì§€ê°€ ì ê¹ ë³´ì´ëŠ” ì˜¤ë¥˜ê°€ ìˆì–´ì„œ, ë¹ˆ ê¸€ í•˜ë‚˜ ì¼ë”ë‹ˆ ì˜¤ë¥˜ í•´ê²°ë¨
    
    st.session_state.messages.append(HumanMessage(type="human", content=query))
    st.chat_message("human").markdown(query
                                      , unsafe_allow_html=True
                                      )

    with st.chat_message("ai"):
        print(f"messages: {st.session_state.messages}")
        
        streaming_eeve_llm = ChatOllama(model="EEVE-Korean-Instruct-10.8B-v1.0:latest"
                            , temperature=0
                            , callbacks=[StreamCallback(st.empty(), initial_text="")]
                            )
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""
ë„ˆëŠ” ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ìœ ëŠ¥í•œ ì—…ë¬´ ë³´ì¡°ìì•¼.
ì˜¤ëŠ˜ ë‚ ì§œëŠ” {current_date}ì•¼. ë¬´ì—‡ì„ ìš”ì²­ë°›ì•˜ì„ ë•Œ í•­ìƒ ì •í™•í•œ í˜„ì¬ ë‚ ì§œë¥¼ ì œê³µí•´ì¤˜.
í•­ìƒ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜.
ë‹¤ìŒ ì§ˆë¬¸ì— ìµœì„ ì„ ë‹¤í•´ì„œ ëŒ€ë‹µí•´ì¤˜.
""")
            , MessagesPlaceholder(variable_name="messages"),
        ])
        streaming_chain = prompt | streaming_eeve_llm
        
        response = ""
        print(f"selected_option_name: {st.session_state.selected_option_name}")
        
        if st.session_state.selected_option_name is None:
            with st.spinner(""):
                # response = chain.invoke({"messages": st.session_state.messages}, {"callbacks": [stream_handler]})
                response = streaming_chain.invoke({"messages": st.session_state.messages})
                print(f"chain.invoke / response: {response}")
                time.sleep(0.1)
                st.session_state.messages.append(AIMessage(type="ai", content=response.content))
        else:
            try:
                with st.spinner("ê²€ìƒ‰ ì¤‘ì´ì—ìš” ğŸ”"):
                    response = agent_chain.invoke({"messages": st.session_state.messages})
                    print(f"agent_chain.invoke / response: {response}")
                    st.markdown(response
                                , unsafe_allow_html=True
                                )
                    time.sleep(0.1)
                    
                    if (len(st.session_state.retrieved_docs) <= 0):
                        # ê²€ìƒ‰ ë¬¸ì„œ ì—†ìœ¼ë©´
                        st.session_state.messages.append(AIMessage(type="ai", content=response))
                    else:
                        # ê²€ìƒ‰ ë¬¸ì„œ ìˆìœ¼ë©´ LLM ë‹µë³€ê³¼ ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦
                        grounded_result = perform_groundedness_check(
                            answer=response
                            , context=get_page_contents_string(st.session_state.retrieved_docs)
                            )
                        grounded_label, grounded_color = grounded_result_mapping.get(grounded_result, ("ì•Œ ìˆ˜ ì—†ìŒ", "gray"))
                        grounded_msg = f'<span style="color:gray;">ë¬¸ì„œ ê²€ì¦ ê²°ê³¼:</span> <span style="color:{grounded_color}; font-weight:bold;">{grounded_label}</span>'
                        st.markdown(grounded_msg
                                    , unsafe_allow_html=True
                                    )
                        
                        # ê²€ìƒ‰ ë¬¸ì„œ ì´ˆê¸°í™”
                        st.session_state.retrieved_docs = []
                        
                        llm_resp_and_grounded_msg = f"{response}\n\n{grounded_msg}"
                        st.session_state.messages.append(AIMessage(type="ai", content=llm_resp_and_grounded_msg))
            except Exception as e:
                print(f"error: {e}")
                st.write("ê²€ìƒ‰ ì‹¤íŒ¨í–ˆì–´ìš”, ì•„ëŠ” ë§Œí¼ ë‹µë³€í• ê²Œìš” ğŸ« ")
                # ê²€ìƒ‰ ì‹¤íŒ¨í–ˆê¸° ë•Œë¬¸ì— LLMí•œí…Œ ê·¸ëƒ¥ ë¬¼ì–´ë³´ê¸°
                with st.spinner(""):
                    # response = chain.invoke({"messages": st.session_state.messages}, {"callbacks": [stream_handler]})
                    response = streaming_chain.invoke({"messages": st.session_state.messages})
                    print(f"chain.invoke / response: {response}")
                    time.sleep(0.1)
                    st.session_state.messages.append(AIMessage(type="ai", content=response.content))