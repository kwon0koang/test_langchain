import os
import sys

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage
from langchain_openai import ChatOpenAI
import json
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
import streamlit as st
from streamlit.runtime.state import SessionStateProxy
from langchain_core.vectorstores import VectorStoreRetriever
import time
from langchain_core.runnables import RunnablePassthrough
from langchain.tools.retriever import create_retriever_tool
from langchain_core.pydantic_v1 import BaseModel, Field
from constants import MY_NEWS_INDEX, MY_PDF_INDEX
from embeddings import embeddings
from callbacks import StreamCallback
from tools import tools, TOOL_AUTO, SAVED_NEWS_SEARCH_TOOL_NAME, PDF_SEARCH_TOOL_NAME, WEB_SEARCH_TOOL_NAME

st.title("ê¶Œë´‡ ğŸ¤–")

eeve_llm = ChatOllama(model="EEVE-Korean-Instruct-10.8B-v1.0:latest", temperature=0)
llama_llm = ChatOllama(model="llama3:8b", temperature=0)
# qwen2_llm = ChatOllama(model="qwen2:latest", temperature=0)

# ==========================================================================================================================================================================================

# ì‚¬ì´ë“œë°”ì— tool ëª©ë¡ ì…‹íŒ…

options = [
    (None, 'ì„ íƒ ì•ˆí•¨'),
    (SAVED_NEWS_SEARCH_TOOL_NAME, 'ì €ì¥ëœ ë‰´ìŠ¤ ê²€ìƒ‰'),
    (PDF_SEARCH_TOOL_NAME, 'ì €ì¥ëœ PDF ê²€ìƒ‰'),
    (WEB_SEARCH_TOOL_NAME, 'WEB ê²€ìƒ‰'),
    (TOOL_AUTO, 'ë„êµ¬ ìë™ ì„ íƒ (BETA)'),
]

# ì˜µì…˜ëª… ì¶”ì¶œ
option_names, option_display_names = zip(*options)

if 'selected_option_name' not in st.session_state:
    st.session_state.selected_option_name = None

# ì„ íƒëœ ì˜µì…˜ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_selected_option():
    selected_option_display_name = st.session_state.selected_option_display_name
    st.session_state.selected_option_name = next(name for name, display_name in options if display_name == selected_option_display_name)

# ì‚¬ì´ë“œë°”ì— selectbox ìƒì„±
selected_option_display_name = st.sidebar.selectbox(
    'ë„êµ¬ ğŸ› ï¸',
    option_display_names,
    on_change=update_selected_option,
    key='selected_option_display_name'
)

# ==========================================================================================================================================================================================

# ì í•©í•œ tool ì¶”ì¶œ ìœ„í•œ í”„ë¡¬í”„íŠ¸
prompt_for_select_tool = ChatPromptTemplate.from_messages([
    ("system", """
You have "tools" that can answer "question".
Using "tools" as a guide, choose a "tool" that can answer "question".
Without saying anything else, say the "tool_name" of the selected "tool" in English.
If there is no appropriate "tool", say "None".

<tools>
{tools}
</tools>

<question>
{question}
</question>

# answer :
"""
    )
])

# tool ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_tools(query):
    tool_info = [{"tool_name": tool.name, "tool_description": tool.description} for tool in tools]
    print(f"get_tools / {tool_info}")
    return json.dumps(tool_info, ensure_ascii=False)

# toolëª…ìœ¼ë¡œ retriever ì°¾ê¸°
def get_retriever_by_tool_name(name) -> VectorStoreRetriever:
    print(f"get_retriever_by_tool_name / name: {name}")
    for tool in tools:
        if tool.name == name:
            # print(tool.func) # functools.partial(<function _get_relevant_documents at 0x1487dd6c0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x317e52ea0>, search_kwargs={'k': 5}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\n\n')
            return tool.func.keywords['retriever']
    return None

# ì í•©í•œ tool ì¶”ì¶œ ìœ„í•œ ì²´ì¸
chain_for_select_tool = (
    {"tools": get_tools, "question": RunnablePassthrough()}
    | prompt_for_select_tool 
    # | llm
    | llama_llm
    # | qwen2_llm
    | StrOutputParser()
    )

# ==========================================================================================================================================================================================

agent_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë„ˆëŠ” ìœ ëŠ¥í•œ ì—…ë¬´ ë³´ì¡°ìì•¼.
contextë¥¼ ì‚¬ìš©í•´ì„œ questionì— ëŒ€í•œ ë‹µì„ ë§í•´ì¤˜.
ì •ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³ ë§Œ í•´.
""")
    , MessagesPlaceholder(variable_name="messages"),
    ("human", "{user_input}")
])

retrieved_docs = []
def get_page_contents_with_metadata(docs) -> str: 
    global retrieved_docs
    retrieved_docs = docs
    
    result = ""
    
    for i, doc in enumerate(docs):
        if i > 0:
            result += "\n\n"
            
        if 'url' in doc:
            # Web ê²€ìƒ‰
            result += f"## ë³¸ë¬¸: {doc['content']}\n### ì¶œì²˜: {doc['url']}"
        else:
            # Vector DB ê²€ìƒ‰
            result += f"## ë³¸ë¬¸: {doc.page_content}\n### ì¶œì²˜: {doc.metadata['source']}"
    
    return result


# ë¬¸ì„œ ê²€ìƒ‰ í›„ ìƒˆ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
def get_new_messages_after_doc_retrieval(messages_dict) -> dict:
    print("========================")
    print(f"messages_dict: {messages_dict}") # {'messages': [HumanMessage(content='ë¼ë§ˆ3 ì„±ëŠ¥ì€?')]}
    messages = messages_dict["messages"]
    print(f"messages: {messages}")
    last_human_message = messages[-1].content
    print(f"last_human_message: {last_human_message}")
    
    selected_tool = ""
    if TOOL_AUTO == st.session_state.selected_option_name:
        selected_tool = chain_for_select_tool.invoke(last_human_message) # LLM í•œí…Œ tool ì„ íƒí•˜ê²Œ í•˜ê¸°
        print(f"chain_for_select_tool.invoke ê²°ê³¼ / selected_tool: {selected_tool}")
        # í›„ë³´ì • Start ============================
        # ê°€ë” "ì›¹ ê²€ìƒ‰(saved_news_search)" ìš”ëŸ° í˜•ì‹ìœ¼ë¡œ ë§í•¨ ã…œ
        if PDF_SEARCH_TOOL_NAME in selected_tool:
            selected_tool = PDF_SEARCH_TOOL_NAME
        elif SAVED_NEWS_SEARCH_TOOL_NAME in selected_tool:
            selected_tool = SAVED_NEWS_SEARCH_TOOL_NAME
        else:
            selected_tool = ""
        # í›„ë³´ì • End ============================
    else:
        selected_tool = st.session_state.selected_option_name
    retriever = get_retriever_by_tool_name(selected_tool)
    
    if retriever is None:
        raise ValueError(f"{selected_tool} retrieverê°€ ì—†ì–´ìš”")
    
    global retrieved_docs
    retrieved_docs = retriever.invoke(last_human_message)
    
    print(f"retrieved_docs: {retrieved_docs}")
    
    new_human_message = HumanMessage(content=f"""
<question>
{last_human_message}
</question>

<context>
{get_page_contents_with_metadata(retrieved_docs)}
</context>

# answer :
""")
    
    messages_without_last = messages[:-1]
    return {"messages": messages_without_last, "user_input": new_human_message}

# ì¶œì²˜ ê°€ì ¸ì˜¤ê¸°
def get_metadata_sources(docs) -> str: 
    sources = set()
    
    for doc in docs:
        if 'url' in doc:
            # Web ê²€ìƒ‰
            sources.add(doc['url'])
        else:
            # Vector DB ê²€ìƒ‰
            source = doc.metadata['source']
            is_pdf = source.endswith('.pdf')
            if (is_pdf):
                file_path = doc.metadata['source']
                file_name = os.path.basename(file_path)
                source = f"[{file_name} ({int(doc.metadata['page']) + 1}í˜ì´ì§€)](file://{file_path})"
            sources.add(source)
        
    return "\n\n".join(sources)

# AI ë©”ì‹œì§€ ë’¤ì— ì¶œì²˜ ë¶™ì´ê¸°
def parse(ai_message: AIMessage) -> str:
    """Parse the AI message and add source."""
    return f"{ai_message.content}\n\n[ì¶œì²˜]\n\n{get_metadata_sources(retrieved_docs)}"

agent_chain = (
    get_new_messages_after_doc_retrieval
    | agent_prompt
    | eeve_llm
    | parse
)

# ==========================================================================================================================================================================================

if "messages" not in st.session_state:
    # st.session_state.messages = [AIMessage(type="ai", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")]
    st.session_state.messages = []

# 10ê°œê¹Œì§€ë§Œ í‘œì‹œë˜ë„ë¡ ë©”ì‹œì§€ ìˆ˜ ì œí•œ
MAX_MESSAGES_COUNT = 10
if len(st.session_state.messages) >= MAX_MESSAGES_COUNT:
    st.session_state.messages = st.session_state.messages[2:]

for msg in st.session_state.messages:
    print(f"for msg in st.session_state.messages / msg.content: {msg.content}")
    st.chat_message(msg.type).write(msg.content)

query = st.chat_input()
if query:
    st.write("") # agent_chain ê²€ìƒ‰ ì¤‘ ë©”ì‹œì§€ ë„ìš¸ ë•Œ, ì´ì „ ë©”ì‹œì§€ê°€ ì ê¹ ë³´ì´ëŠ” ì˜¤ë¥˜ê°€ ìˆì–´ì„œ, ë¹ˆ ê¸€ í•˜ë‚˜ ì¼ë”ë‹ˆ ì˜¤ë¥˜ í•´ê²°ë¨
    
    st.session_state.messages.append(HumanMessage(type="human", content=query))
    st.chat_message("human").write(query)

    with st.chat_message("ai"):
        print(f"messages: {st.session_state.messages}")
        
        streaming_eeve_llm = ChatOllama(model="EEVE-Korean-Instruct-10.8B-v1.0:latest"
                            , temperature=0
                            , callbacks=[StreamCallback(st.empty(), initial_text="")]
                            )
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful, professional assistant named ê¶Œë´‡. answer me in Korean no matter what"),
            MessagesPlaceholder(variable_name="messages"),
        ])
        streaming_chain = prompt | streaming_eeve_llm
        
        response = ""
        print(f"selected_option_name: {st.session_state.selected_option_name}")
        
        if st.session_state.selected_option_name:
            try:
                with st.spinner("ê²€ìƒ‰ ì¤‘ì´ì—ìš” ğŸ”"):
                    response = agent_chain.invoke({"messages": st.session_state.messages})
                    print(f"agent_chain.invoke / response: {response}")
                    st.markdown(response)
                    time.sleep(0.1)
                    st.session_state.messages.append(AIMessage(type="ai", content=response))
            except Exception as e:
                print(f"error: {e}")
                st.write("ì ì ˆí•œ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”, ì•„ëŠ” ë§Œí¼ ë‹µë³€í• ê²Œìš” ğŸ« ")
                # ë„êµ¬ ì°¾ê¸°ì— ì‹¤íŒ¨í–ˆê¸° ë•Œë¬¸ì— LLMí•œí…Œ ê·¸ëƒ¥ ë¬¼ì–´ë³´ê¸°
                with st.spinner(""):
                    # response = chain.invoke({"messages": st.session_state.messages}, {"callbacks": [stream_handler]})
                    response = streaming_chain.invoke({"messages": st.session_state.messages})
                    print(f"chain.invoke / response: {response}")
                    time.sleep(0.1)
                    st.session_state.messages.append(AIMessage(type="ai", content=response.content))
        else:
            with st.spinner(""):
                # response = chain.invoke({"messages": st.session_state.messages}, {"callbacks": [stream_handler]})
                response = streaming_chain.invoke({"messages": st.session_state.messages})
                print(f"chain.invoke / response: {response}")
                time.sleep(0.1)
                st.session_state.messages.append(AIMessage(type="ai", content=response.content))